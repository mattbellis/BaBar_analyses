{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4db8eb-3d54-4930-9e8d-0f78b00d7263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "from hist import Hist\n",
    "\n",
    "import babar_analysis_tools as bat\n",
    "from analysis_variables import *\n",
    "import myPIDselector\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784d123-55ea-4b94-87f0-9cbf14d0d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "start= time.time()\n",
    "\n",
    "## My laptop\n",
    "#topdir= \"/Users/josieswann/BaBar_analyses/BNV_pLambda/\"\n",
    "\n",
    "## Bellis computer\n",
    "topdir= \"/home/bellis/babar_data/bnv_plambda\"\n",
    "#topdir= \"/home/bellis/babar_data/bnv_plambda_bnc\"\n",
    "\n",
    "\n",
    "filename= f\"{topdir}/Background_and_signal_SP_modes_Only_Run_1.parquet\"\n",
    "#filename= f\"{topdir}/Background_and_signal_SP_modes_All_runs.parquet\" ## this won't run on mine \n",
    "#filename= f\"{topdir}/Background_and_signal_SP_modes_BNC_Only_Run_1.parquet\"\n",
    "#filename= f\"{topdir}/Background_and_signal_SP_modes_BNC_All_runs.parquet\"\n",
    "\n",
    "data= ak.from_parquet(filename)\n",
    "\n",
    "print(f\"Took {time.time()-start} seconds\")\n",
    "\n",
    "IS_MC= True\n",
    "\n",
    "#Collision data \n",
    "\n",
    "#filename = f'{topdir}/Background_SP_modes_Only_Run_1.parquet'\n",
    "filename = f'{topdir}/Data_Only_Run_1_BLINDED.parquet'\n",
    "#filename = f'{topdir}/Data_All_runs_BLINDED.parquet'\n",
    "#filename = f'{topdir}/Data_All_runs_BLINDED.parquet'\n",
    "#filename = f'{topdir}/Data_BNC_Only_Run_1.parquet'\n",
    "#filename = f'{topdir}/Data_BNC_All_runs.parquet'\n",
    "\n",
    "start= time.time()\n",
    "data_collision= ak.from_parquet(filename)\n",
    "\n",
    "print(f\"took {time.time()-start} seconds\")\n",
    "\n",
    "print(type(data_collision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b5bee-4526-4b92-8c10-7b35a8edb80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_information= pd.read_csv(\"dataset_statistics.csv\")\n",
    "cs_data= pd.read_csv(\"SP_cross_sections_and_labels.csv\")\n",
    "\n",
    "no_notes= cs_data.drop([\"Uncertainty\",\"Note: cross sections found at https://babar-wiki.heprc.uvic.ca/bbr_wiki/index.php/Physics/Cross_sections,_luminosities,_and_other_vital_stats\"], axis= 1)\n",
    "no_notes\n",
    "\n",
    "sp= data[\"spmode\"]\n",
    "\n",
    "splist= np.unique(sp.to_list())\n",
    "splist\n",
    "\n",
    "dcuts= bat.get_final_masks(data, region_definitions= region_definitions)\n",
    "\n",
    "print([dcuts.keys()])\n",
    "print()\n",
    "\n",
    "for key in dcuts.keys():\n",
    "    print(f'{key:3d} {dcuts[key][\"name\"]}')\n",
    "\n",
    "dcuts[3]\n",
    "\n",
    "bat.fill_new_entry_with_tag_side_B(data)\n",
    "data[\"BtagSideMes\"]\n",
    "bat.fill_new_entry_with_tag_side_B(data_collision)\n",
    "data_collision[\"BtagSideMes\"]\n",
    "\n",
    "all_hists= bat.create_empty_histograms(hist_defs)\n",
    "\n",
    "bkg_spmodes= [\"998\",\"1005\",\"3981\",\"1235\",\"1237\"]\n",
    "sig_spmodes= [\"-999\"]\n",
    "\n",
    "spmodes= bkg_spmodes+sig_spmodes\n",
    "\n",
    "weights= {}\n",
    "for sp in spmodes: \n",
    "    weights[sp]= bat.scaling_value(int(sp),dataset_information=dataset_information, cs_data= cs_data, plot= False, verbose= False)\n",
    "    #weights[sp]=1\n",
    "\n",
    "weights[\"-999\"]= 1000 #scales signal higher \n",
    "weights[\"0\"]= 1 #idk what this is for;;; ASK\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3e798-d6e2-4cc6-94ca-4b8bdbf85256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask_event= dcuts[1][\"event\"]\n",
    "#mask_event= dcuts[2][\"event\"]\n",
    "mask_event= dcuts[3][\"event\"]\n",
    "#mask_event= dcuts[4][\"event\"] ## individual cuts\n",
    "#mask_event= dcuts[-1][\"event\"] ## all cuts\n",
    "\n",
    "#mask_event= dcuts[1][\"event\"] & dcuts[2][\"event\"] & dcuts[3][\"event\"] & dcuts[4][\"event\"] ## combo of cuts\n",
    "mask_event= dcuts[1][\"event\"] & dcuts[2][\"event\"] & dcuts[3][\"event\"] ## combo of cuts\n",
    "\n",
    "### ASK WHAT THESE MEAN\n",
    "#tag= \"EARLY_CUT\"\n",
    "tag= \"FINAL_CUTS\"\n",
    "\n",
    "mask= mask_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cae3cc-7635-41a2-9ad2-bbfac05800be",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['spmode', 'BpostFitMes', 'BpostFitDeltaE', 'Lambda0_unc_Mass', \\\n",
    "          'BtagSideMes', 'BSphr', 'BThrust', 'BCosThetaS', \\\n",
    "          'R2', 'R2All', \\\n",
    "          'thrustMag', 'thrustMagAll', 'thrustCosTh', 'thrustCosThAll', 'sphericityAll', \\\n",
    "          'BCosSphr', 'BCosThetaT', 'BCosThrust', 'BLegendreP2', 'BR2ROE', 'BSphrROE', \\\n",
    "          'BThrustROE']\n",
    "\n",
    "ak_array_type= type(data[\"spmode\"])\n",
    "\n",
    "df_dict={}\n",
    "for var in subset: \n",
    "    x= data[mask][var] ##in each event, cut on the above cuts and pull out the info from each of the variables listed above\n",
    "    if type(x[0]) == ak_array_type:\n",
    "        x= ak.flatten(data[mask][var])\n",
    "    df_dict[var] = x\n",
    "\n",
    "df_out= pd.DataFrame.from_dict(df_dict)\n",
    "\n",
    "outfilename= f\"output_variables_{tag}.parquet\"\n",
    "df_out.to_parquet(outfilename)\n",
    "\n",
    "df= df_out\n",
    "\n",
    "df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1762539-eba3-4bc4-9f82-7c9f1b944d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter= df[\"spmode\"]== \"-999\"\n",
    "\n",
    "g= sns.PairGrid(df[filter].sample(500), vars= [\"BpostFitMes\",\"BpostFitDeltaE\"], hue= \"spmode\")\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_offdiag(sns.scatterplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e91dd6-dc3b-4019-8eb5-b5b2f72fe7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = df['spmode'] != '-999'\n",
    "columns= df.columns\n",
    "\n",
    "#g = sns.PairGrid(df[filter].sample(500), vars=['BpostFitMes', 'BpostFitDeltaE'], hue='spmode')\n",
    "g = sns.PairGrid(df[filter].sample(50), vars=columns[1:6], hue='spmode')\n",
    "\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_offdiag(sns.scatterplot)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41199e-3940-4811-897f-e6e701da26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names= columns[1:] ##exclude spmode\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5471c-2a64-43d7-9781-a4b554b38b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcuts= bat.get_final_masks(data, region_definitions= region_definitions)\n",
    "\n",
    "for key,val in dcuts.items():\n",
    "    print(key, val['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fd7e1-34f7-49cc-af0f-1163ebd85538",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut1 = dcuts[1]['event']\n",
    "print(len(cut1))\n",
    "\n",
    "len(dcuts[3]['event'][cut1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5b9d7-886d-4e61-bfea-8ee66c01932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['spmode', 'BpostFitMes', 'BpostFitDeltaE', 'Lambda0_unc_Mass', \\\n",
    "      'BtagSideMes', 'BSphr', 'BThrust', 'BCosThetaS', \\\n",
    "      'R2', 'R2All', \\\n",
    "      'thrustMag', 'thrustMagAll', 'thrustCosTh', 'thrustCosThAll', 'sphericityAll', \\\n",
    "      'BCosSphr', 'BCosThetaT', 'BCosThrust', 'BLegendreP2', 'BR2ROE', 'BSphrROE', \\\n",
    "      'BThrustROE']\n",
    "\n",
    "dcuts= bat.get_final_masks(data, region_definitions= region_definitions)\n",
    "\n",
    "mask_event= dcuts[1][\"event\"]# & dcuts[2][\"event\"] & dcuts[3][\"event\"] \n",
    "mask= mask_event\n",
    "\n",
    "df_sp = bat.dump_awkward_to_dataframe(data[mask], fields_to_dump=subset)\n",
    "\n",
    "# Put the cuts into the dataframe \n",
    "cut1 = dcuts[1]['event'] # This is the main cut that gets rid of duplicates\n",
    "cuts_to_add = [2, 3, 4, 6, -1]\n",
    "for cut in cuts_to_add:\n",
    "    bools = dcuts[cut]['event']\n",
    "    colname = f'cut_{cut}'\n",
    "    print(colname, len(bools[cut1]), bools[cut1])\n",
    "\n",
    "    df_sp[colname] = bools[cut1]\n",
    "\n",
    "###################################\n",
    "# Collision\n",
    "dcuts_col= bat.get_final_masks(data_collision, region_definitions= region_definitions)\n",
    "\n",
    "mask_event= dcuts_col[1][\"event\"]# & dcuts[2][\"event\"] & dcuts[3][\"event\"] \n",
    "mask= mask_event\n",
    "\n",
    "df_col = bat.dump_awkward_to_dataframe(data_collision[mask], fields_to_dump=subset)\n",
    "\n",
    "# Put the cuts into the dataframe \n",
    "cut1 = dcuts_col[1]['event'] # This is the main cut that gets rid of duplicates\n",
    "cuts_to_add = [2, 3, 4, 6, -1]\n",
    "for cut in cuts_to_add:\n",
    "    bools = dcuts_col[cut]['event']\n",
    "    colname = f'cut_{cut}'\n",
    "    print(colname, len(bools[cut1]), bools[cut1])\n",
    "\n",
    "    df_col[colname] = bools[cut1]\n",
    "\n",
    "\n",
    "df_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d0e8a0-996e-413a-92b7-1e69874c1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062efd28-5302-4340-9da2-f4e84650dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_sp['cut_-1'] == True\n",
    "df_sp[mask].hist('BpostFitMes', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1a9a5-5583-4ba5-9ad9-da44f5895159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_maker(df, sig_spmode=\"-999\", bkg_spmode= \"998\", n_sig_bkg=[1000,1000], \\\n",
    "                columns_to_drop=[\"spmode\",\"BpostFitMes\",\"BpostFitDeltaE\",\"Lambda0_unc_Mass\"], \\\n",
    "                test_size_pct= 0.4, activator= \"relu\", solve_model= \"adam\", model_filename=None): \n",
    "            \n",
    "    columns= df.columns\n",
    "\n",
    "    print(\"columns...\")\n",
    "    print(columns)\n",
    "    print()\n",
    "\n",
    "    feature_names= columns[1:] ##exclude spmode\n",
    "    #print(feature_names)\n",
    "    filter_sig= df[\"spmode\"]==sig_spmode\n",
    "    filter_bkg= df[\"spmode\"]==bkg_spmode\n",
    "    \n",
    "    df_sig= df[filter_sig].dropna().sample(n_sig_bkg[0])\n",
    "    df_bkg= df[filter_bkg].dropna().sample(n_sig_bkg[1])\n",
    "\n",
    "    print(len(df_sig), len(df_bkg))\n",
    "    \n",
    "    df_ML= pd.concat([df_sig,df_bkg])\n",
    "    \n",
    "    x=df_ML.drop(columns= columns_to_drop)\n",
    "    y=df_ML[\"spmode\"]\n",
    "\n",
    "    print(\"y\")\n",
    "    print(y)\n",
    "    print()\n",
    "    \n",
    "    #print(\"Hi there\")\n",
    "\n",
    "    feature_names= x.columns ##disc vars\n",
    "    labels= y.unique() ##diff sp modes\n",
    "    \n",
    "    print(\"Training features:\")\n",
    "    print(feature_names)\n",
    "    print()\n",
    "    \n",
    "    print(\"Labels (Outcome):\")\n",
    "    print(labels)\n",
    "    print()\n",
    "    \n",
    "    print(\"The dataset (x) is the numbers without column names---\")\n",
    "    print(\"The variable y is truth info about the data (signal or bkg)\")\n",
    "\n",
    "    x_train, x_test, y_train, y_test= train_test_split(x,y, test_size= test_size_pct, random_state= 4)\n",
    "    scaler= StandardScaler()\n",
    "\n",
    "    # Look in \"tips for practical use\"\n",
    "    # https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "    x_train= scaler.fit_transform(x_train)\n",
    "    x_test= scaler.transform(x_test)\n",
    "    \n",
    "    model = MLPClassifier(max_iter= 1000, random_state= 3, activation= activator, solver= solve_model ) #n_iter_no_change= 15)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    model.feature_names = feature_names\n",
    "    \n",
    "    workspace = {}\n",
    "    workspace['model'] = model\n",
    "    workspace['x_train'] = x_train\n",
    "    workspace['y_train'] = y_train\n",
    "    workspace['x_test'] = x_test\n",
    "    workspace['y_test'] = y_test\n",
    "\n",
    "    if model_filename is not None:\n",
    "        joblib.dump(workspace, model_filename)\n",
    "\n",
    "    return workspace\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf59ad-75fa-41c8-913f-8206d19891f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_variables(df):\n",
    "\n",
    "    #########################################################################\n",
    "    # Plot the variables for the different spmodes\n",
    "    #########################################################################\n",
    "\n",
    "    print(\"Plotting the training variables...\")\n",
    "    nvars = len(df.columns)\n",
    "\n",
    "    nrows = 5\n",
    "    ncols = int(nvars / nrows) + 1\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows = nrows, ncols = ncols)    # axes is 2d array (3x3)\n",
    "    axes = axes.flatten()         # Convert axes to 1d array of length 9\n",
    "    fig.set_size_inches(ncols*3, nrows*3)\n",
    "\n",
    "    for ax, col in zip(axes, df.columns):\n",
    "      sns.histplot(df, x=col, ax = ax, hue='spmode', stat='density', common_norm=False)\n",
    "      ax.set_title(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #########################################################################\n",
    "    # Plot the correlation matrices\n",
    "    #########################################################################\n",
    "\n",
    "    spmodes_in_file = df['spmode'].unique()\n",
    "    # Drop the cuts columns\n",
    "    cols = df_temp.columns\n",
    "\n",
    "    cols_temp = []\n",
    "    for col in cols:\n",
    "        #print(col)\n",
    "        if col[0:3]!='cut':\n",
    "            cols_temp.append(col)\n",
    "    cols_temp\n",
    "\n",
    "    for spmode in spmodes_in_file:\n",
    "        print(f\"Making the correlation matrix for SP-{spmode}...\")\n",
    "        fig,ax = plt.subplots(figsize=(16,16))\n",
    "        mask = df_temp['spmode'] == spmode\n",
    "        \n",
    "        sns.heatmap(df_temp[mask][cols_temp].drop(columns=['spmode']).corr(), center=0, cmap='coolwarm', annot=True, fmt='.2f', annot_kws={\"size\": 8})\n",
    "        plt.title(f'Correlation matrix SP {spmode}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3865a-d3a0-4655-8709-20d72eb7e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cols = df_temp.columns\n",
    "\n",
    "cols = cols.to_list()\n",
    "\n",
    "cols_temp = []\n",
    "for col in cols:\n",
    "    #print(col)\n",
    "    if col[0:3]!='cut':\n",
    "        cols_temp.append(col)\n",
    "\n",
    "cols_temp\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209d667-5b70-4205-b948-52a9d3200da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sp['spmode'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94caf4b9-e29a-4418-b2c5-6079f5e46baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sp.columns\n",
    "df_sp['spmode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ff046-8954-451e-a2fa-1fe736c7b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a temporary dataframe with the cuts\n",
    "\n",
    "mask = (df_sp['cut_2']==True) & (df_sp['cut_3']==True)\n",
    "\n",
    "df_temp = df_sp[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2e628-360b-4ec8-b6d0-300d22b628a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_mask = (df_temp['spmode']=='-999') | (df_temp['spmode']=='998')\n",
    "\n",
    "plot_training_variables(df_temp[sp_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e90c8-6da8-4be5-81c8-e3b936b8903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig,ax = plt.subplots(figsize=(16,16))\n",
    "#mask = df_temp['spmode'] == '998'\n",
    "##corr = df_temp[mask].drop(columns=['spmode']).corr()\n",
    "##corr.style.background_gradient(cmap='coolwarm', axis=None).format(precision=2)\n",
    "\n",
    "#sns.heatmap(df_temp[mask].drop(columns=['spmode']).corr(), center=0, cmap='coolwarm', annot=True, fmt='.2f', annot_kws={\"size\": 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e6a25-0e62-4eb0-a307-ae2cad459f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = df_sp['spmode'] == '-999'\n",
    "#corr = df_sp[mask].drop(columns=['spmode']).corr()\n",
    "#corr.style.background_gradient(cmap='coolwarm', axis=None).format(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac670b31-f2db-4651-8874-5e72d3f4b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['cut_2', 'cut_3', 'cut_4', 'cut_6', 'cut_-1', \\\n",
    "                   \"spmode\",\"BpostFitMes\",\"BpostFitDeltaE\",\"Lambda0_unc_Mass\", \\\n",
    "                  'BSphr', 'BThrust']\n",
    "#[\"spmode\",\"BpostFitMes\",\"BpostFitDeltaE\",\"Lambda0_unc_Mass\",\"BtagSideMes\"]\n",
    "\n",
    "model_filename = \"TEST_MODEL_SAVE.pkl\"\n",
    "workspace = model_maker(df_temp, columns_to_drop=columns_to_drop, \\\n",
    "                                                      n_sig_bkg=[2000, 2000], model_filename=model_filename)#, \n",
    "\n",
    "model = workspace['model']\n",
    "x_train = workspace['x_train']\n",
    "y_train = workspace['y_train']\n",
    "x_test = workspace['x_test']\n",
    "y_test = workspace['y_test']\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122352cd-96e7-45f4-8882-355263c7f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29c60d-b838-4306-9085-e1018a9ce4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = joblib.load('TEST_MODEL_SAVE.pkl')\n",
    "#workspace\n",
    "\n",
    "model = workspace['model']\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a009e8b-e3b7-46d2-a215-d42922060401",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace\n",
    "\n",
    "### Not done yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707aaaae-9eca-4990-9d12-047b91bdb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#def model_training_quality(model, x_train, y_train, x_test, y_test):\n",
    "def model_training_quality(workspace):\n",
    "    model = workspace['model']\n",
    "    x_train = workspace['x_train']\n",
    "    y_train = workspace['y_train']\n",
    "    x_test = workspace['x_test']\n",
    "    y_test = workspace['y_test']\n",
    "    \n",
    "    #model\n",
    "    ###################################################################\n",
    "    # Get the predictions for the training and testing samples\n",
    "    ###################################################################\n",
    "    decisions = []\n",
    "    for X, y in ((x_train, y_train), (x_test, y_test)):\n",
    "\n",
    "      # Use the outcome to select the truth information (>0.5 or <0.5)\n",
    "      d1 = model.predict_proba(X[y == '998'])[:, 1]\n",
    "      d2 = model.predict_proba(X[y == '-999'])[:, 1]\n",
    "      decisions += [d1, d2]\n",
    "    \n",
    "    # Use this for the histogram ranges\n",
    "    low = min(np.min(d) for d in decisions)\n",
    "    high = max(np.max(d) for d in decisions)\n",
    "    low_high = (low, high)\n",
    "    \n",
    "    \n",
    "    #print(decisions)\n",
    "    ###################################################################\n",
    "    # Make a plot of the training sample predictions\n",
    "    ###################################################################\n",
    "\n",
    "    bins = 50\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(decisions[0],\n",
    "              color='r', alpha=0.5, range=low_high, bins=bins,\n",
    "              histtype='stepfilled', density=True,\n",
    "              label='Bkg (train)')\n",
    "    plt.hist(decisions[1],\n",
    "              color='b', alpha=0.5, range=low_high, bins=bins,\n",
    "              histtype='stepfilled', density=True,\n",
    "              label='Sig (train)')\n",
    "    \n",
    "    \n",
    "    # Make a plot with error bars for the testing samples\n",
    "    hists, bins = np.histogram(decisions[2],density=True,\n",
    "                              bins=bins, range=low_high)\n",
    "    scale = len(decisions[2]) / sum(hists)\n",
    "    err = np.sqrt(hists * scale) / scale\n",
    "    \n",
    "    width = (bins[1] - bins[0])\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    \n",
    "    plt.errorbar(center, hists, yerr=err, fmt='o', c='r', label='Bkg (test)')\n",
    "    \n",
    "    hists, bins = np.histogram(decisions[3],density=True,\n",
    "                              bins=bins, range=low_high)\n",
    "    scale = len(decisions[2]) / sum(hists)\n",
    "    err = np.sqrt(hists * scale) / scale\n",
    "    \n",
    "    plt.errorbar(center, hists, yerr=err, fmt='o', c='b', label='Sig (test)')\n",
    "    \n",
    "    plt.xlabel(\"Classifer output\")\n",
    "    plt.ylabel(\"Arbitrary units\")\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    # Confusion matrix\n",
    "    # Testing the model i.e. predicting the labels of the test data.\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Evaluating the results of the model\n",
    "    accuracy = accuracy_score(y_test,y_pred)*100 ### returns the fraction of correctly classified samples \n",
    "    confusion_mat = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "    print(\"Accuracy for Neural Network is:\",accuracy)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_mat)\n",
    "    \n",
    "    tot_correct= confusion_mat[1][1] + confusion_mat[0][0]\n",
    "    tot_wrong= confusion_mat[1][0] + confusion_mat[0][1]\n",
    "    \n",
    "    #print(tot_correct/(tot_correct+tot_wrong))\n",
    "    \n",
    "    ## The accuracy score is the total number classified correctly over the total number of classifications \n",
    "\n",
    "\n",
    "    # Turn this into a dataframe\n",
    "    matrix_df = pd.DataFrame(confusion_mat)\n",
    "    \n",
    "    # Plot the result\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    \n",
    "    sns.set(font_scale=1.3)\n",
    "    \n",
    "    sns.heatmap(matrix_df, annot=True, fmt=\"g\", ax=ax, cmap=\"magma\")\n",
    "    \n",
    "    #labels = df['target_names'].tolist()\n",
    "    labels = ['998', '-999'] # NEED TO FIX THIS SO IT IS NOT HARDCODED\n",
    "    \n",
    "    # Formatting details here\n",
    "    # Set axis titles\n",
    "    ax.set_title('Confusion Matrix - MLP')\n",
    "    ax.set_xlabel(\"Predicted label\", fontsize =15)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylabel(\"True Label\", fontsize=15)\n",
    "    ax.set_yticklabels(labels, rotation = 0)\n",
    "    #plt.show()\n",
    "\n",
    "    # ROC\n",
    "\n",
    "    decisions = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    ###################################################################\n",
    "    # Compute ROC curve and area under the curve\n",
    "    ###################################################################\n",
    "\n",
    "    sig_bkg = np.ones_like(y_test, dtype=int)\n",
    "    sig_bkg[y_test=='-999'] = 0\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(sig_bkg, decisions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(fpr, tpr, lw=1, label='ROC (area = %0.2f)' % (roc_auc))\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90250c50-5417-4978-acf0-0a572717eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = joblib.load('TEST_MODEL_SAVE.pkl')\n",
    "\n",
    "model_training_quality(workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb5b93-20c2-49d0-b98b-9420fdc3f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\"\n",
    "def ML_FOM_Calculator(workspace, df_col):\n",
    "\n",
    "    model = workspace['model']\n",
    "    x_train = workspace['x_train']\n",
    "    y_train = workspace['y_train']\n",
    "    x_test = workspace['x_test']\n",
    "    y_test = workspace['y_test']\n",
    "\n",
    "    # Get the training vars\n",
    "    training_vars = model.feature_names\n",
    "    df_col_tmp = df_col[training_vars]\n",
    "\n",
    "    print(training_vars)\n",
    "\n",
    "    #### FOR SP \n",
    "    #for i in threshold:\n",
    "    #    output_df= see_stuff(sig_samp= sig_samp,bkg_samp= bkg_samp, thresh= i, verbose= False, df=MC_data, df_col=coll_data)\n",
    "\n",
    "    # 3. Scale the test data using the same scaler\n",
    "    scaler = StandardScaler()\n",
    "    x_dummy = scaler.fit_transform(x_train)\n",
    "\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    x_sp_proba = model.predict_proba(x_test)\n",
    "\n",
    "\n",
    "    x_col_proba = model.predict_proba(df_col_tmp)\n",
    "    \n",
    "    #### FOR COLLISION\n",
    "    #y_proba_col_sig = model.predict_proba()\n",
    "    \n",
    "    #sp998= sp_data[\"spmode\"]== \"998\"\n",
    "    #N_bkg = len(sp_998_df[sp998]) ## total number of background events (sp 998) \n",
    "    #signal_before= len(sp_999_df)\n",
    "    #signal_after= len(sp_999_df)\n",
    "    #efficiency = signal_after/signal_before ## the accuracy of the model after training with the SP \n",
    "    #fom = efficiency(threshold)/(np.sqrt(N_bkg(threshold)+sig_disc/2))\n",
    "    #return fom\n",
    "    return 0\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5327845-9c7d-41c1-a5c1-fbc784c21ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col.dropna(inplace=True)\n",
    "ML_FOM_Calculator(workspace, df_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c4eaf6-a790-4333-a43f-6aede6e26268",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = workspace['model']\n",
    "\n",
    "x_train = workspace['x_train']\n",
    "y_train = workspace['y_train']\n",
    "y_test = workspace['y_test']\n",
    "\n",
    "#x_train\n",
    "#y_train\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0cdcf5-1264-48f8-b6d5-f63f94e269a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_names = ['hi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e14e55e-7972-4541-9339-d2e9b55e6398",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df3b1b3-723f-4970-a69e-61f387281e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0799d5ce-9848-4082-9610-28fe36cf535c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ad3cd-85e2-4218-9d8d-3915b57293ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
